{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AeroPace/ME-592/blob/main/README_PointNet_ModelNet40_Updates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8lC9ZctXcz2"
      },
      "source": [
        "**HW 4 - PointNet:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authors:** Alexander Krekelberg, Danial Pace, Prahlad Pandav\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Note:** In order to get the GitHub Code for PointNet to work with both ShapeNet and ModelNet40 Datasets several things had to be changed and/or done manually in order to make it work in google colab. This partially has to do with the age of the code and installation links for various packages having issues.\n",
        "\n",
        "\n",
        "The first thing that needs to be done is that you need to download all of the GitHub files for PointNet and upload everything in the pointnet-pointnet master folder (you are not uploading either of the pointnet-master folders that you get but instead the files that are in them such as provider.py or the part_seg folder) to the files section of google colab runtime. You need to make sure that the data in the files uploaded to runtime (doc, models, part_seg, etc...) are all in folders with the same names as in the GitHub (you want the same filestructure as is in the second pointnet-master folder you get after unzipping the downloaded file). Once you do this then follow the instructions for both of the two datasets below.\n",
        "\n",
        "If you want to save time, then zip the files that are inside of the pointnet-master folder and name the zip file the same name as the folder itself and upload just the zip file to runtime (don't zip the folder itself). Then you can run the following code to unzip the code and delete the zip files:"
      ],
      "metadata": {
        "id": "q62QqsrorXij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip files\n",
        "!unzip doc.zip -d doc\n",
        "!unzip models.zip -d models\n",
        "!unzip part_seg.zip -d part_seg\n",
        "!unzip sem_seg.zip -d sem_seg\n",
        "!unzip utils.zip -d utils\n",
        "\n",
        "#Remove zip files\n",
        "!rm doc.zip\n",
        "!rm models.zip\n",
        "!rm part_seg.zip\n",
        "!rm sem_seg.zip \n",
        "!rm utils.zip"
      ],
      "metadata": {
        "id": "i7lgfWl58Gvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**ModelNet40**\n"
      ],
      "metadata": {
        "id": "fqm9jMMtsHzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get the ModelNet40 dataset to run you need to make the following changes to the code in the files mentioned below:\n",
        "> \n",
        "**Note:** Do not run the following blocks of code below in this session, just read the instructions provided in them and do what they say"
      ],
      "metadata": {
        "id": "FLQ0tauZt86V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLiUncXOofzj"
      },
      "outputs": [],
      "source": [
        "#Replace the code on lines 12-17 in /content/provider.py with the following code\n",
        "\n",
        "if not os.path.exists(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048')):\n",
        "    www = 'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip'\n",
        "    zipfile = os.path.basename(www)\n",
        "    os.system('wget %s --no-check-certificate; unzip %s' % (www, zipfile))\n",
        "    os.system('mv %s %s' % (zipfile[:-4], DATA_DIR))\n",
        "    os.system('rm %s' % (zipfile))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the code on line 5 in /content/train.py with the following code\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "metadata": {
        "id": "6IIAoNGlKeWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After replacing the original code with the above code you need to run the following lines of code in order to revert tensorflow to an previous version (you do not need to rerun this code if you have already done it for the other dataset):"
      ],
      "metadata": {
        "id": "S0O968MNuujd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to run this before running /content/train.py\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow===1.15.0\n",
        "import tensorflow as tf\n",
        "tf.version"
      ],
      "metadata": {
        "id": "6qiQexWVuu8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to change the hyperparameters of the code, go into /content/train.py and alter the values for the code between lines 20 and 30. Then run the following code to train and then evaluate your model:"
      ],
      "metadata": {
        "id": "vYiHbhxV7SLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "yD-gR6zau-fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "id": "vgxCqZXJu5Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**ShapeNet**"
      ],
      "metadata": {
        "id": "nCWQ-qhDvBsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get the ShapeNet dataset to run you need to make the following changes to the code in the files mentioned below:\n",
        "\n",
        "> \n",
        "**Note:** Do not run the following four blocks of code below in this session, just read the instructions provided in them and do what they say\n"
      ],
      "metadata": {
        "id": "tKNgHn4eApvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the code on lines 12-17 in /content/provider.py with the following code\n",
        "\n",
        "if not os.path.exists(os.path.join(DATA_DIR, 'shapenetcore_partanno_v0')):\n",
        "    www = 'https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_v0.zip'\n",
        "    zipfile = os.path.basename(www)\n",
        "    os.system('wget %s --no-check-certificate; unzip %s' % (www, zipfile))\n",
        "    os.system('mv %s %s' % (zipfile[:-4], DATA_DIR))\n",
        "    os.system('rm %s' % (zipfile))"
      ],
      "metadata": {
        "id": "6_xO1WMiuMny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace all of the code in /content/part_seg/downloaded_data.sh with the following\n",
        "\n",
        "#!/bin/bash\n",
        "# Download original ShapeNetPart dataset (around 1GB)\n",
        "wget --no-check-certificate https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_v0.zip\n",
        "unzip shapenetcore_partanno_v0.zip\n",
        "rm shapenetcore_partanno_v0.zip\n",
        "# Download HDF5 for ShapeNet Part segmentation (around 346MB)\n",
        "wget --no-check-certificate https://shapenet.cs.stanford.edu/media/shapenet_part_seg_hdf5_data.zip\n",
        "unzip shapenet_part_seg_hdf5_data.zip\n",
        "rm shapenet_part_seg_hdf5_data.zip"
      ],
      "metadata": {
        "id": "3OwR8goXKhSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the code on line 19 in /content/part_seg/train.py with the following code:\n",
        "#(Note that the number of epochs must be at least 10 to get the code to work)\n",
        "\n",
        "parser.add_argument('--epoch', type=int, default=10, help='Epoch to run [default: 50]')"
      ],
      "metadata": {
        "id": "eiUHzM-VQGxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the code on line 14 in /content/part_seg/test.py with the following code:\n",
        "#(Note that the number in epoch_10 must be an increment of 10 and should be something below or the same as the total number of epochs):\n",
        "\n",
        "parser.add_argument('--model_path', default='train_results/trained_models/epoch_10.ckpt', help='Model checkpoint path')"
      ],
      "metadata": {
        "id": "ylP6tzJ-KnNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After replacing the original code with the above code you need to run the following lines of code in order to revert tensorflow to an previous version (you do not need to rerun this code if you have already done it for the other dataset):"
      ],
      "metadata": {
        "id": "zLFhuNd7ATP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to run this before running /content/train.py\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow===1.15.0\n",
        "import tensorflow as tf\n",
        "tf.version"
      ],
      "metadata": {
        "id": "88YyY6JUEzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running and replacing the above code you need to run the following line of code:"
      ],
      "metadata": {
        "id": "GQ5wCpg8EyWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sh part_seg/download_data.sh"
      ],
      "metadata": {
        "id": "Ds33p4WwATyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the above code and creating the hdf5_data and PartAnnotation folders in /content, move them into the /content/part_seg folder so that you get the following data paths:\n",
        "*   /content/part_seg/hdf5_data\n",
        "*   /content/part_seg/PartAnnotation"
      ],
      "metadata": {
        "id": "ZaXTNwsEuDRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to change the hyperparameters of the code, go into /content/part_seg/train.py and alter the values for the code between lines 17 and 22. Then run the following code to train and then evaluate your model:"
      ],
      "metadata": {
        "id": "Jd-XVaGtAUt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python part_seg/train.py"
      ],
      "metadata": {
        "id": "3mbXH42cAbcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python part_seg/test.py"
      ],
      "metadata": {
        "id": "uz1CZ9i3HPhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW4_PointNet.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
